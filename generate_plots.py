import random
import os
import numpy as np
import matplotlib.pyplot as plt

from simulated_annealing import simulated_annealing
from hill_climbing import hill_climbing

def generate_traces(method, directory):
    # Iterate through 50 different seeds)
    seeds = [i+1 for i in range(50)]
    # Allow the algorithm to run for 100 seconds max on any iteration
    cutoff = 100
    
    # Repeat for both datasets large_1 and large_3
    for instance in ["large_1", "large_3"]:
        # pull in dataset
        values = []
        weights = []
        f = open("DATASET/large_scale/{}".format(instance), "r")
        _, capacity = map(int, f.readline().split())
        for line in f.readlines():
            value, weight = line.split()
            values.append(float(value))
            weights.append(float(weight))
        f.close()
        
        for seed in seeds:
            # Run simulated annealing
            if method == "ls1":
                return  # ADD METHOD HERE
            # Run hill climbing
            elif method == "ls2":
                best_value, best_solution, trace = hill_climbing(values, weights, capacity, cutoff, seed)
            else:
                print("Please use a local search method to graph QRTD and SQD plots")
                return
            
            # Generate trace file for algorithm in new directory for generating plots
            filename = "{}{}_{}_{}_{}.trace".format(directory, instance, method, cutoff, seed)
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            with open(filename, "w") as f:
                for step in trace:
                    f.write(", ".join(map(str, step)) + "\n")    

def generate_plots(method, regen):
    # Set new directory name solely for generating plots
    directory = "plot_traces/"
    # Generate new trace files if the directory is empty or doesn't exist
    if not os.path.exists(directory) or not os.listdir(directory):
        generate_traces(method, directory)
    # If flagged, delete existing trace files for the ls method and regenerate them
    elif regen:
        for root, dirs, files in os.walk(directory):
            for name in files:
                if method in name:
                    os.remove(os.path.join(root, name))
        generate_traces(method, directory)

    # get large_1 solution
    f = open("DATASET/large_scale_solution/{}".format("large_1"), "r")
    opt_sol_1 = int(f.read())
    f.close()

    # get large_3 solution
    f = open("DATASET/large_scale_solution/{}".format("large_3"), "r")
    opt_sol_3 = int(f.read())
    f.close()

    traces_1 = []
    traces_3 = []
    # iterate through every generated trace file in the plot_traces directory
    for file in os.listdir(directory):
        # Only look at trace files for the current method
        if method not in file:
            continue
        
        # Pull out timesteps and values from the trace files
        trace_timesteps = []
        trace_values = []
        f = open(os.path.join(directory, file), "r")
        for line in f.readlines():
            timestep, value = line.split(", ")
            trace_values.append(float(value))
            trace_timesteps.append(float(timestep))
        f.close()
        # Set to np arrays for data manipulation
        trace_timesteps = np.array(trace_timesteps)
        trace_values = np.array(trace_values)

        # Separate the trace data between the two instances used
        if "large_1" in file:
            traces_1.append((trace_timesteps, trace_values))
        elif "large_3" in file:
            traces_3.append((trace_timesteps, trace_values))

    # Create a plot with 4 subplots (QRTD and SQD for each of the instances)
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)
    # Combine the traces together with applicable metadata - method, traces, opt_sol, subplot1, subplot2
    both_traces = [tuple(("large_1", traces_1, opt_sol_1, ax1, ax2)), tuple(("large_3", traces_3, opt_sol_3, ax3, ax4))]
    # Preset qualities to look at
    qualities = np.array([0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0])
    # Preset timesteps to look at
    times = np.array([0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100])

    # Iterate through both groups of traces (for both instances)
    for metadata in both_traces:
        instance = metadata[0]
        traces = metadata[1]
        opt_sol = metadata[2]
        qrtd = metadata[3]
        sqd = metadata[4]

        # Fix each quality for QRTD
        for q in qualities:
            # Create a probability vector
            probs = np.zeros_like(times)
            # Iterate through each trace generated by the method for the current instance
            for trace in traces:
                # Pull out timesteps, values, and calculate relative error
                timesteps = trace[0]
                values = trace[1]
                relative_errors = (opt_sol - values)/opt_sol

                # Get indices of the largest timesteps less than the given times
                time_idx = np.searchsorted(timesteps, times, side="left")
                # Adjust indices to not overflow
                time_idx = np.where(time_idx == len(relative_errors), time_idx-1, time_idx)

                # Find largest relative errors found by a certain timestep
                best_rel_errors = relative_errors[time_idx]
                # Increment probability vector when a relative error is found that is less than the threshold
                probs = probs + np.where(best_rel_errors <= q, 1, 0)
            # Calculate the probability of the algorithm reaching a certain quality over multiple iterations
            probs = probs / len(traces)
            # Plot the probability vector against the times examined
            qrtd.plot(times, probs, label = q)
        # Configurue QRTD diagram
        qrtd.legend()
        qrtd.set_xlabel("Run-Time [CPU sec]")
        qrtd.set_ylabel("P(solve)")
        qrtd.set_title("{} QRTD".format(instance))
        qrtd.set_xscale('log')

        # Fix each time for SQD
        for t in times:
            # Create a probability vector
            probs = np.zeros_like(qualities)
            # Iterate through each trace generated by the method for the current instance
            for trace in traces:
                # Pull out timesteps, values, and calculate relative error
                timesteps = trace[0]
                values = trace[1]
                relative_errors = (opt_sol - values)/opt_sol
                
                # Get indices of the smallest relative errors greater than the given qualities
                qualities_idx = np.searchsorted(-relative_errors, -qualities, side="left")
                # Adjust indices to -1 if the quality is never reached
                qualities_idx = np.where(qualities_idx >= len(timesteps), -1, qualities_idx)
                
                # Get the earliest times when the qualities are reached (inf if the qualities are never reached)
                best_times = np.where(qualities_idx == -1, np.inf, timesteps[qualities_idx])
                # Incremenet prbability vector when a relative error is found that is less than the threshold within a certain time
                probs = probs + np.where(best_times <= t, 1, 0)
            # Calculate the probability of the algorithm reaching a certain quality within a certain time
            probs = probs / len(traces)
            # Plot the probability vector against the qualities examined
            sqd.plot(qualities, probs, label = t)
        # Configurue SQD diagram
        sqd.legend()
        sqd.set_xlabel("Relative Solution Quality")
        sqd.set_title("{} SQD".format(instance))
        sqd.set_ylabel("P(solve)")
    #Configure and generate plot
    plt.suptitle("{} Charts".format(method))
    plt.show()